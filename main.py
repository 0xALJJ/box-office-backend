import os
import json
import requests
from datetime import datetime
from bs4 import BeautifulSoup
from supabase import create_client, Client
from openai import OpenAI

# ä» GitHub çš„ç¯å¢ƒå˜é‡é‡Œè¯»å–é’¥åŒ™
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
TARGET_URL = os.environ.get("TARGET_URL") # è¿™æ˜¯ä½ æ‰‹åŠ¨è¾“å…¥çš„æ–‡ç« é“¾æ¥

if not all([SUPABASE_URL, SUPABASE_KEY, OPENAI_API_KEY, TARGET_URL]):
    print("âŒ é”™è¯¯ï¼šç¼ºå°‘å¿…è¦çš„ç¯å¢ƒå˜é‡æˆ–æ–‡ç« é“¾æ¥")
    exit(1)

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
openai_client = OpenAI(api_key=OPENAI_API_KEY)

# --- æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---
def fetch_article(url):
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(resp.text, 'html.parser')
        # ç®€å•ç²—æš´ï¼šæŠ“å–æ‰€æœ‰æ®µè½
        return "\n".join([p.text for p in soup.find_all('p')])[:8000]
    except Exception as e:
        print(f"æŠ“å–å¤±è´¥: {e}")
        return None

def get_deadline_analyst_id():
    # æŸ¥æ‰¾æˆ–åˆ›å»º Deadline åˆ†æå¸ˆ
    res = supabase.table("analysts").select("id").eq("outlet", "Deadline").execute()
    if res.data: return res.data[0]['id']
    new = supabase.table("analysts").insert({"name":"Anthony","outlet":"Deadline"}).execute()
    return new.data[0]['id']

def ai_parse(text, movie_name):
    prompt = f"""
    ä»ä¸‹æ–‡ä¸­æå–ç”µå½±ã€Š{movie_name}ã€‹çš„åŒ—ç¾é¦–å‘¨æœ«ç¥¨æˆ¿é¢„æµ‹ã€‚
    è¿”å›çº¯JSONæ ¼å¼: {{"min":æ•°å­—(ç™¾ä¸‡), "max":æ•°å­—, "avg":æ•°å­—}}ã€‚
    è‹¥æ— æ•°æ®åˆ™å…¨è¿”å›0ã€‚æ–‡æœ¬: {text}
    """
    resp = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}]
    )
    content = resp.choices[0].message.content.replace("```json","").replace("```","").strip()
    return json.loads(content)

# --- ä¸»ç¨‹åº ---
def main():
    print(f"ğŸš€ å¼€å§‹å¤„ç†é“¾æ¥: {TARGET_URL}")
    
    # 1. æ‰¾æ­£åœ¨è¿½è¸ªçš„ç”µå½±
    movies = supabase.table("movies").select("*").eq("status", "Tracking").execute().data
    if not movies:
        print("âš ï¸ æ²¡æœ‰æ­£åœ¨è¿½è¸ª (Tracking) çš„ç”µå½±ï¼Œè¯·å…ˆåœ¨æ•°æ®åº“æ·»åŠ ç”µå½±")
        return

    analyst_id = get_deadline_analyst_id()
    
    # 2. æŠ“å–æ–‡ç« 
    content = fetch_article(TARGET_URL)
    if not content: return

    # 3. éå†æ¯éƒ¨ç”µå½±ï¼Œé—® AI æ–‡ç« é‡Œæœ‰æ²¡æœ‰æåˆ°å®ƒ
    for movie in movies:
        print(f"ğŸ” æ­£åœ¨åˆ†æç”µå½±: {movie['title_en']}")
        data = ai_parse(content, movie['title_en'])
        
        if data['avg'] > 0:
            print(f"âœ… æ‰¾åˆ°æ•°æ®: {data}")
            # è®¡ç®—å€’è®¡æ—¶
            release = datetime.strptime(movie['release_date'], "%Y-%m-%d").date()
            days = (release - datetime.now().date()).days
            
            # å†™å…¥æ•°æ®åº“
            supabase.table("predictions").insert({
                "movie_id": movie['id'],
                "analyst_id": analyst_id,
                "scraped_date": str(datetime.now().date()),
                "days_to_release": days,
                "forecast_min": data['min'],
                "forecast_max": data['max'],
                "forecast_avg": data['avg']
            }).execute()
            print("ğŸ’¾ å·²ä¿å­˜åˆ° Supabase")
        else:
            print("âŒ æ–‡ç« æœªæåŠè¯¥ç”µå½±")

if __name__ == "__main__":
    main()
